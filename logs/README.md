# Logs Directory

This directory contains JSON log files from the tender scraping tasks.

## Log Files

### Scraping Logs
- **Format**: `scrape_YYYYMMDD_HHMMSS.json`
- **Generated by**: `tasks/scrape_tenders.py`
- **Frequency**: Every time the scraper runs (typically hourly)

### Cron Logs
- **Format**: `cron.log`
- **Generated by**: Cron job execution
- **Content**: Standard output and errors from scheduled tasks

## Log Structure

Each scraping log file contains:

```json
{
  "task": "scrape_tenders",
  "start_time": "2025-11-08T16:00:00",
  "end_time": "2025-11-08T16:05:23",
  "duration_seconds": 323.45,
  "country_code": "UK",
  "parameters": {
    "limit": 100,
    "stage": null,
    "days_back": 7
  },
  "status": "success",
  "records_fetched": 100,
  "records_inserted": 85,
  "records_duplicates": 15,
  "errors": []
}
```

## Log Retention

Logs are kept indefinitely for audit purposes. You may want to implement log rotation:

```bash
# Keep only last 30 days of logs
find logs/ -name "scrape_*.json" -mtime +30 -delete
```

## Viewing Logs

### Latest scraping log
```bash
ls -t logs/scrape_*.json | head -1 | xargs cat | jq .
```

### All logs from today
```bash
cat logs/scrape_$(date +%Y%m%d)*.json | jq .
```

### Summary of all scraping runs
```bash
jq -s '[.[] | {time: .start_time, status: .status, fetched: .records_fetched, inserted: .records_inserted}]' logs/scrape_*.json
```

## Monitoring

Monitor scraping health with:

```bash
# Check for errors
grep -l '"status": "error"' logs/scrape_*.json

# Count total records inserted today
jq -s 'map(.records_inserted) | add' logs/scrape_$(date +%Y%m%d)*.json
```
